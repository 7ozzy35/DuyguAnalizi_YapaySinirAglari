{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf423a6-b7d0-491e-ac35-19b532c98f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix,  ConfusionMatrixDisplay,roc_curve, roc_auc_score,auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense,Input, Embedding,LSTM,Dropout,Conv1D, MaxPooling1D, GlobalMaxPooling1D,Dropout,Bidirectional,Flatten,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d578089-8515-4ff3-9c39-71d81ee60665",
   "metadata": {},
   "source": [
    "## Veriyi DataFrame'e dönüştürme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8df586f-bff1-48fb-83b8-774b78998e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('IMDB_Dataset.csv')\n",
    "df= data.copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb4e43-cb8f-4f0e-96d9-97842999e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d0375-2ca6-4865-b2cd-79eb1acbf261",
   "metadata": {},
   "source": [
    "İki sütun halinde görüntülediğimiz veri setimizde istenmeyen noktalama ve analiz aşamasında işimize yaramayacak, eğitim kalitesini azaltacak gereksiz kelimeler bulunmakta. Bunlar ileriki aşamalarda temizlenecek ve ortaya daha duru bir veri seti çıkacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9edc97d-d669-47c3-bb5b-23dc04fb72bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7533631-d772-4574-a36b-cc3b52b64573",
   "metadata": {},
   "source": [
    "Bu ifadede \"top\" en çok bulunan yorumu \"freq\" ise tüm veri setindeki en yaygın yorumun frekansını ifade eder. Yani bu durumda veri setinde top yorumdan 5 adet bulunduğunu anlayabiliriz. 49582 tane de benzersiz yorum vardır. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b920ad93-9cd9-4f1c-9e3d-1228a75bafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Her sentiment'in sayısını verir\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "\n",
    "#sentiment sayılarını çizdirir\n",
    "plt.figure(figsize=(8, 6))\n",
    "sentiment_counts.plot(kind='bar', color=['lightcoral', 'mediumpurple'])\n",
    "plt.title('Sentiment Analysis of Reviews')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26a3ce-6902-43be-a121-85f087ec9788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a1e8788-8990-4260-980e-6ab841113d57",
   "metadata": {},
   "source": [
    "## Veri Temizleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa895462-24d4-46d6-9aa5-9f15900bf1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ad7f5-4016-4830-b1b3-4391954ce22e",
   "metadata": {},
   "source": [
    "Öncelikle null değerleri kontrol ettik ancak herhangi bir null değer bulunmuyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c7314-33d8-4e05-8079-8a52f716ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb23d70b-7ecb-42f6-b153-2d90bd8ffd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace = True) #Tekrarlanan değerlerin silinmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631735ed-b94f-45cf-a080-0fa7f8c2f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sentiment = df.sentiment.map({ 'negative': 0, 'positive': 1 }) \n",
    "# Sentiment sütunundaki stringler sayısal değerlere dönüştürülür.\n",
    "X = df.review.tolist()\n",
    "y = df.sentiment.tolist() # Var olan iki sütun birer listeye dönüştürülür."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18a7b6a-040d-4ae1-9dbe-c29d9f5efd17",
   "metadata": {},
   "source": [
    "## Verinin eğitim ve test setlerine bölünmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b6543a-7218-4879-8ac3-b17bc704465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae81ac11-8fe0-475d-a37d-2bee5ac0584e",
   "metadata": {},
   "source": [
    "# Ön İşleme Aşamaları"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9399eb1b-a472-458f-b306-e4a0a6a25c52",
   "metadata": {},
   "source": [
    "## Noktalama işaretlerini temizleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a02a9af-f968-4b20-991f-5f18d20e2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "# str.maketrans fonksiyonu ile noktalama işaretlerini boşluk karakterlerine dönüştüren bir tablo oluşturulur. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a8a413-55be-46a0-9191-87b420b9b696",
   "metadata": {},
   "source": [
    "## HTML etiketlerinin temizlenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2219fd7-9049-44b0-9f33-38ad801285dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = re.compile(r'<[^>]+>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10d81b4-dc47-40be-ba93-3db54ee98115",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean = []\n",
    "X_test_clean = []\n",
    "for i, text in enumerate(X_train):\n",
    "    text2 = text.lower()   # Küçük harfe çevirme\n",
    "    text2 = text2.replace('\\n', '')   #Tüm yeni satır ('\\n') karakterlerini kaldırır\n",
    "    text2 = clean.sub('', text2)   #html etiketlerini kaldırır\n",
    "    text2 = text2.translate(translator)  #Noktalama işaretlerini boşluklarla değiştirme\n",
    "    X_train_clean.append(text2)\n",
    "\n",
    "for i, text in enumerate(X_test):\n",
    "    text2 = text.lower()\n",
    "    text2 = text2.replace('\\n', '')\n",
    "    text2 = clean.sub('', text2)\n",
    "    text2 = text2.translate(translator)\n",
    "    X_test_clean.append(text2)\n",
    "\n",
    "X_train_clean = np.array(X_train_clean)\n",
    "X_test_clean = np.array(X_test_clean)\n",
    "\n",
    "X_train = X_train_clean\n",
    "X_test = X_test_clean\n",
    "\n",
    "X_train_copy = X_train.copy()\n",
    "X_test_copy= X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b33e5e3-1866-428a-8cb8-e2b03a05699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459fe4a4-d135-43b4-af0f-10125503f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "\n",
    "# Tüm metinleri birleştir\n",
    "train_text = \" \".join(X_train)\n",
    "test_text = \" \".join(X_test)\n",
    "\n",
    "# WordCloud oluşturma fonksiyonu\n",
    "def generate_wordcloud(text, max_words=50, title=None):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=max_words).generate(text)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        plt.title(title, fontsize=20)\n",
    "\n",
    "# Kelime bulutlarını yan yana çizme\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Eğitim verileri kelime bulutu\n",
    "axs[0].imshow(WordCloud(width=800, height=400, background_color='white', max_words=50).generate(train_text), interpolation='bilinear')\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('Training Data WordCloud', fontsize=20)\n",
    "\n",
    "# Test verileri kelime bulutu\n",
    "axs[1].imshow(WordCloud(width=800, height=400, background_color='white', max_words=50).generate(test_text), interpolation='bilinear')\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('Test Data WordCloud', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e3edb2-6106-4efe-a0f7-4d9e5e3a3114",
   "metadata": {},
   "source": [
    "## Tokenization İşlemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf55f0e-0b93-4b94-81a3-1cd11a102440",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 15000\n",
    "tokenizer = Tokenizer(num_words=top_words)\n",
    "tokenizer.fit_on_texts(X_train) #Tokenizer nesnesi, X_train veri kümesi üzerinde eğitilir.\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test) # Veri kümelerindeki metinler, Tokenizer nesnesi tarafından sayısal dizilere dönüştürülür. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0fe4be-c28c-49d1-99a5-d119a81bf80e",
   "metadata": {},
   "source": [
    "## Padding işlemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b66b52-bc6b-42e4-ab8c-58ef41e398f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words, padding='post')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words, padding='post')\n",
    "# 'pad_sequences' çeşitli uzunluktaki metin dizilerini aynı uzunluğa getirerek, bunları sabit boyutlu tensörlere dönüştürmek için kullanılır. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ed5e6-e66c-4679-b004-b526d0d71b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236c1fe6-dd38-4a03-afd3-bb25d91bf39f",
   "metadata": {},
   "source": [
    "## Model kurulumu"
   ]
  },
  {
   "cell_type": "raw",
   "id": "875d6129-6815-4b3b-80b0-25866cb2a702",
   "metadata": {},
   "source": [
    "1.Sequential sınıfı, katmanları sıralı olarak ekleyebileceğimiz bir model oluşturur.\n",
    "2.Embedding, metin verilerini sayısal vektörlere dönüştürür.\n",
    "3.BatchNormalization, eğitim sırasında her mini-batch için girişleri normalize eder. Bu, ağın daha hızlı ve daha kararlı bir şekilde öğrenmesine yardımcı olur.\n",
    "4.Dropout, belirli bir oranda nöronları rastgele sıfırlar, bu da modelin overfitting yapmasını engeller.\n",
    "5.Conv1D, 1 boyutlu konvolüsyonel katman. Metin verileri üzerinde konvolüsyon işlemi yapar. 32 farklı filtre uygulanır.\n",
    "6.MaxPooling1D: 1 boyutlu maksimum havuzlama katmanı. Konvolüsyon çıktılarının boyutunu küçültür.\n",
    "7.Bidirectional, çift yönlü LSTM katmanı, girdi dizilerini hem ileri hem de geri yönde işler.\n",
    "8.LSTM(64), tek yönlü LSTM katmanı, 64 birim içerir. Bu katman tam diziyi değil, son zaman adımının çıktısını döndürür.\n",
    "9.Dense, tam bağlantılı (fully connected) katman. Çıktı katmanıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fbdc62-d9cd-4d6f-9431-f8740bb502de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1277b5-c8b8-48ed-abc0-d2d4b5b9818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2caecf-d576-4587-937a-7a48bca82476",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = new_model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640aae28-42c9-47d6-a615-bfad4752a37d",
   "metadata": {},
   "source": [
    "#### Eğitim sırasında elde edilen değerlerle doğruluk(accuracy) ve kayıp(loss) grafiği"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b59594-737a-4612-bc3b-3518c9005bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, name):\n",
    "  plt.plot(history.history[name])\n",
    "  plt.plot(history.history['val_'+name])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(name)\n",
    "  plt.legend([name, 'val_'+name])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ec15e-ada9-4f1f-969c-f8e44c3ada12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, \"accuracy\")\n",
    "plot_history(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6508ab-477d-4481-9b98-3d931ebe7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Modelin test seti üzerindeki performansı:\")\n",
    "result = model.evaluate(X_test,y_test)\n",
    "print(dict(zip(model.metrics_names, result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac5185-74f4-4e2b-8035-741069058241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "  lines = []\n",
    "  with open(filename, 'r') as file:\n",
    "    for line in file:\n",
    "      lines.append(line.strip())\n",
    "  return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01963de6-5a8e-4955-8c3e-5430167f71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = read_file('negative.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2aad73-2299-425f-bdfa-05a9facea8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_list = []\n",
    "clean = re.compile(r'<[^>]+>')\n",
    "for i, text in enumerate(neg):\n",
    "    tmp_text = text.lower()\n",
    "    tmp_text = tmp_text.replace('\\n', '')\n",
    "    tmp_text = clean.sub('', tmp_text)\n",
    "    tmp_text = tmp_text.translate(translator)\n",
    "    neg_list.append(tmp_text)\n",
    "\n",
    "neg_list = np.array(neg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2312ff3-3a53-46db-b2af-f2934f41605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_list = tokenizer.texts_to_sequences(neg_list)\n",
    "neg_list = sequence.pad_sequences(neg_list, maxlen=max_words, padding='post')\n",
    "prediction = model.predict(neg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a871d9f-e9b1-4c99-8852-014d60755f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_below_05 = np.sum(prediction < 0.5)\n",
    "count_above_05 = np.sum(prediction >= 0.5)\n",
    "\n",
    "# Sonuçları ekrana yazdırma\n",
    "print(f\"negative: {count_below_05}\")\n",
    "print(f\"positive: {count_above_05}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bd47b4-5d39-4dce-ad17-b78fd9fdafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = read_file(\"positive.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d377913-9e7a-4bfb-90ac-51341bd802b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = []\n",
    "clean = re.compile(r'<[^>]+>')\n",
    "for i, text in enumerate(pos):\n",
    "    tmp_text = text.lower()\n",
    "    tmp_text = tmp_text.replace('\\n', '')\n",
    "    tmp_text = clean.sub('', tmp_text)\n",
    "    tmp_text = tmp_text.translate(translator)\n",
    "    pos_list.append(tmp_text)\n",
    "\n",
    "pos_list = np.array(pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd14b30-df81-4bec-9130-dc36a03bef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = tokenizer.texts_to_sequences(pos_list)\n",
    "pos_list = sequence.pad_sequences(pos_list, maxlen=max_words, padding='post')\n",
    "prediction = model.predict(pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10521e4-a731-4eb6-9d4c-c87d07d83060",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_below_05 = np.sum(prediction < 0.5)\n",
    "count_above_05 = np.sum(prediction >= 0.5)\n",
    "\n",
    "# Sonuçları ekrana yazdırma\n",
    "print(f\"negative: {count_below_05}\")\n",
    "print(f\"positive: {count_above_05}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b67cb3-7427-4e9b-afdf-78e37e74feec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
